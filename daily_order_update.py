"""
è®¢å•çŠ¶æ€æ¯æ—¥è‡ªåŠ¨æ›´æ–°è„šæœ¬
åŠŸèƒ½ï¼šæ¯å¤©å®šæ—¶ä»é›¶æ˜Ÿå¹³å°APIè·å–æœ€æ–°è®¢å•çŠ¶æ€ï¼Œæ›´æ–°
æ–°å¢åŠŸèƒ½ï¼šæ¯æ—¥æ›´æ–°åº“å­˜ã€ä»“åº“ã€åº—é“ºä¿¡æ¯
"""
import os
import sys
import logging
import time
from datetime import datetime, timedelta
from dataoperator import DataOperator
from config import load_config_from_env
from api_use import LingXingAPI
import  traceback
import  json
import  pymysql
from utils import extract_store_name, extract_from_json
# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥æ‚¨çš„æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/daily_order_update.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)
class DailyOrderUpdater:
    """æ¯æ—¥è®¢å•çŠ¶æ€æ›´æ–°å™¨"""
    def __init__(self, app_id, app_secret, db_config):
        """
        åˆå§‹åŒ–æ›´æ–°å™¨
        Args:
            app_id: é›¶æ˜Ÿå¹³å°APP_ID
            app_secret: é›¶æ˜Ÿå¹³å°APP_SECRET
            db_config: æ•°æ®åº“è¿æ¥é…ç½®
        """
        self.api_client = LingXingAPI(app_id, app_secret)
        self.db_config = db_config
        self.data_operator = None
    def connect_database(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.data_operator = DataOperator(self.db_config)
            self.data_operator.connect_db()
            logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    def disconnect_database(self):
        """æ–­å¼€æ•°æ®åº“è¿æ¥"""
        if self.data_operator:
            self.data_operator.disconnect_db()
        logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
    def get_yesterday_time_range(self):
        """
        è·å–æ˜¨å¤©çš„æ—¶é—´èŒƒå›´ï¼ˆç”¨äºæŸ¥è¯¢æ˜¨å¤©æ›´æ–°çš„è®¢å•ï¼‰
        Returns:
            tuple: (start_time, end_time) æ—¶é—´æˆ³
        """
        # è·å–æ˜¨å¤©æ—¥æœŸ
        yesterday = datetime.now() - timedelta(days=1)
        start_time = datetime(yesterday.year, yesterday.month, yesterday.day, 0, 0, 0)
        end_time = datetime(yesterday.year, yesterday.month, yesterday.day, 23, 59, 59)
        start_timestamp = int(start_time.timestamp())
        end_timestamp = int(end_time.timestamp())
        logger.info(f"æŸ¥è¯¢æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}")
        return start_timestamp, end_timestamp
    def get_recent_days_time_range(self, days=1):
        """
        è·å–æœ€è¿‘Nå¤©çš„æ—¶é—´èŒƒå›´
        Args:
            days: æŸ¥è¯¢æœ€è¿‘å¤šå°‘å¤©
        Returns:
            tuple: (start_time, end_time) æ—¶é—´æˆ³
        """
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)
        start_timestamp = int(start_time.timestamp())
        end_timestamp = int(end_time.timestamp())
        logger.info(f"æŸ¥è¯¢æœ€è¿‘{days}å¤©æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}")
        return start_timestamp, end_timestamp
    def fetch_updated_orders(self, days_to_check=1):
        """
        è·å–éœ€è¦æ›´æ–°çš„è®¢å•æ•°æ®
        Args:
            days_to_check: æ£€æŸ¥æœ€è¿‘å¤šå°‘å¤©çš„è®¢å•
        Returns:
            list: è®¢å•æ•°æ®åˆ—è¡¨
        """
        try:
            # è·å–æ—¶é—´èŒƒå›´
            start_time, end_time = self.get_recent_days_time_range(days_to_check)
            # æ„å»ºAPIè¯·æ±‚å‚æ•°
            api_path = "/pb/mp/order/v2/list"
            base_biz_body = {
                "start_time": start_time,
                "end_time": end_time,
                "date_type": "update_time",  # æŒ‰æ›´æ–°æ—¶é—´æŸ¥è¯¢
                "platform_code": [10024],  # æŒ‡å®šå¹³å°
            }
            logger.info("å¼€å§‹è·å–è®¢å•æ›´æ–°æ•°æ®...")
            total_processed = self.api_client.fetch_and_process_order_data_batch(
                api_path, base_biz_body, self.db_config, delay=1
            )
            logger.info(f"è®¢å•æ•°æ®è·å–å®Œæˆï¼Œå…±å¤„ç† {total_processed} æ¡è®°å½•")
            return total_processed > 0
        except Exception as e:
            logger.error(f"è·å–è®¢å•æ•°æ®å¤±è´¥: {e}")
            return False
    def update_store_info(self):
        """
        æ›´æ–°åº—é“ºä¿¡æ¯è¡¨
        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹æ›´æ–°åº—é“ºä¿¡æ¯...")
            # è°ƒç”¨åº—é“ºä¿¡æ¯API
            success = self.api_client.getstoreList(self.db_config)
            if success:
                logger.info("âœ… åº—é“ºä¿¡æ¯æ›´æ–°æˆåŠŸ")
            else:
                logger.error("âŒ åº—é“ºä¿¡æ¯æ›´æ–°å¤±è´¥")
            return success
        except Exception as e:
            logger.error(f"æ›´æ–°åº—é“ºä¿¡æ¯å¤±è´¥: {e}")
            return False
    def update_warehouse_info(self):
        """
        æ›´æ–°ä»“åº“ä¿¡æ¯è¡¨
        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹æ›´æ–°ä»“åº“ä¿¡æ¯...")
            # è°ƒç”¨ä»“åº“ä¿¡æ¯API
            self.api_client.getwarehouseList(self.db_config, type=3)
            logger.info("âœ… ä»“åº“ä¿¡æ¯æ›´æ–°å®Œæˆ")
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°ä»“åº“ä¿¡æ¯å¤±è´¥: {e}")
            return False
    def update_inventory_info(self):
        """
        æ›´æ–°åº“å­˜ä¿¡æ¯è¡¨
        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹æ›´æ–°åº“å­˜ä¿¡æ¯...")
            # è·å–æ‰€æœ‰ä»“åº“ID
            warehouse_ids = self.api_client.getwarehouseids(self.db_config)
            if not warehouse_ids:
                logger.warning("æœªè·å–åˆ°ä»“åº“IDï¼Œè·³è¿‡åº“å­˜æ›´æ–°")
                return False
            # å°†ä»“åº“IDåˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
            wid_str = ",".join([str(wid) for wid in warehouse_ids])
            logger.info(f"è·å–åˆ° {len(warehouse_ids)} ä¸ªä»“åº“ï¼Œå¼€å§‹æ›´æ–°åº“å­˜...")
            # è°ƒç”¨åº“å­˜ä¿¡æ¯API
            success = self.api_client.getinvetoryList(self.db_config, str=wid_str)
            if success:
                logger.info("âœ… åº“å­˜ä¿¡æ¯æ›´æ–°æˆåŠŸ")
            else:
                logger.error("âŒ åº“å­˜ä¿¡æ¯æ›´æ–°å¤±è´¥")
            return success
        except Exception as e:
            logger.error(f"æ›´æ–°åº“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            return False
    def validate_order_status_consistency(self):
        """
        éªŒè¯è®¢å•çŠ¶æ€ä¸€è‡´æ€§ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        ç¡®ä¿ordersè¡¨å’Œplatform_infoè¡¨çš„order_statuså­—æ®µä¸€è‡´
        """
        try:
            if not self.data_operator or not self.data_operator.conn:
                logger.warning("æ•°æ®åº“æœªè¿æ¥ï¼Œè·³è¿‡ä¸€è‡´æ€§éªŒè¯")
                return True
            # æ£€æŸ¥ä¸¤ä¸ªè¡¨çš„è®¢å•çŠ¶æ€æ˜¯å¦ä¸€è‡´
            check_sql = """
            SELECT 
                COUNT(*) as total_orders,
                SUM(CASE WHEN o.order_status = p.order_status THEN 1 ELSE 0 END) as matching_orders
            FROM orders o
            INNER JOIN platform_info p ON o.global_order_no = p.global_order_no
            WHERE o.update_time >= DATE_SUB(NOW(), INTERVAL 7 DAY)
            """
            self.data_operator.cursor.execute(check_sql)
            result = self.data_operator.cursor.fetchone()
            if result and result[0] > 0:
                consistency_rate = (result[1] / result[0]) * 100
                logger.info(f"è®¢å•çŠ¶æ€ä¸€è‡´æ€§æ£€æŸ¥: {consistency_rate:.2f}% ({result[1]}/{result[0]})")
                if consistency_rate < 95:
                    logger.warning("è®¢å•çŠ¶æ€ä¸€è‡´æ€§è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®åŒæ­¥é€»è¾‘")
            return True
        except Exception as e:
            logger.error(f"è®¢å•çŠ¶æ€ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False
    def cleanup_old_data(self, days_to_keep=90):
        """
        æ¸…ç†æ—§æ•°æ®ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        Args:
            days_to_keep: ä¿ç•™å¤šå°‘å¤©çš„æ•°æ®
        """
        try:
            if not self.data_operator or not self.data_operator.conn:
                return False
            # æ¸…ç†90å¤©å‰çš„è®¢å•æ•°æ®ï¼ˆæ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´ï¼‰
            cleanup_sql = """
            DELETE FROM orders 
            WHERE update_time < DATE_SUB(NOW(), INTERVAL %s DAY)
            AND order_status IN ('TRADE_FINISHED', 'TRADE_CLOSED')
            """
            self.data_operator.cursor.execute(cleanup_sql, (days_to_keep,))
            deleted_rows = self.data_operator.cursor.rowcount
            if deleted_rows > 0:
                logger.info(f"æ¸…ç†äº† {deleted_rows} æ¡ {days_to_keep} å¤©å‰çš„å·²å®Œæˆ/å·²å…³é—­è®¢å•")
                self.data_operator.conn.commit()
            return True
        except Exception as e:
            logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            if self.data_operator.conn:
                self.data_operator.conn.rollback()
            return False
    def _update_daily_sales(self, days_back=7, enable_cleanup=False):
        """
        å†…éƒ¨æ–¹æ³•ï¼šæ›´æ–°æ¯æ—¥é”€é‡æ•°æ®ï¼ˆä¸åŒ…å«æ•°æ®åº“è¿æ¥ç®¡ç†ï¼‰
        Args:
            days_back: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®
            enable_cleanup: æ˜¯å¦å¯ç”¨æ•°æ®æ¸…ç†
        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            overall_success = True
            # æ›´æ–°é”€é‡ç»Ÿè®¡æ•°æ®ï¼ˆæŒ‰ä¸åŒç»´åº¦åˆ†åˆ«æ›´æ–°ï¼‰
            update_tasks = [
                {"name": "SKUç»´åº¦é”€é‡", "data_type": "4"},
                {"name": "åº—é“ºç»´åº¦é”€é‡", "data_type": "6"},
                {"name": "ASINç»´åº¦é”€é‡", "data_type": "1"}
            ]
            for task in update_tasks:
                logger.info(f"å¼€å§‹æ›´æ–° {task['name']} æ•°æ®...")
                task_success = self.update_sales_statistics(
                    days_back=days_back,
                    result_type="1",  # é”€é‡
                    date_unit="4",  # æŒ‰æ—¥ç»Ÿè®¡
                    data_type=task['data_type']
                )
                if not task_success:
                    logger.warning(f"{task['name']} æ›´æ–°å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡")
                    overall_success = False
                else:
                    logger.info(f"âœ… {task['name']} æ›´æ–°å®Œæˆ")
                # ä»»åŠ¡é—´çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…APIé™æµ
                time.sleep(2)
            # å¯é€‰ï¼šæ¸…ç†æ—§æ•°æ®
            if enable_cleanup:
                cleanup_success = self.cleanup_old_sales_data(days_to_keep=90)
                if not cleanup_success:
                    logger.warning("é”€é‡æ•°æ®æ¸…ç†å¤±è´¥")
            return overall_success
        except Exception as e:
            logger.error(f"é”€é‡æ•°æ®æ›´æ–°å¤±è´¥: {e}")
            return False
    def _generate_update_report(self, task_results, execution_time, overall_success):
        """
        ç”Ÿæˆæ›´æ–°ä»»åŠ¡æŠ¥å‘Š
        Args:
            task_results: å„ä»»åŠ¡æ‰§è¡Œç»“æœå­—å…¸
            execution_time: æ€»æ‰§è¡Œæ—¶é—´
            overall_success: æ•´ä½“æ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("æ¯æ—¥æ•°æ®æ›´æ–°ä»»åŠ¡æŠ¥å‘Š")
        logger.info("=" * 60)
        success_count = 0
        total_count = 0
        for task_name, result in task_results.items():
            total_count += 1
            status_icon = "âœ…" if result is True else "âš ï¸" if result == "è·³è¿‡" else "âŒ"
            status_text = "æˆåŠŸ" if result is True else "è·³è¿‡" if result == "è·³è¿‡" else "å¤±è´¥"
            logger.info(f"{status_icon} {task_name}: {status_text}")
            if result is True:
                success_count += 1
        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
        logger.info("-" * 40)
        logger.info(f"ä»»åŠ¡å®Œæˆæƒ…å†µ: {success_count}/{total_count} ({success_rate:.1f}%)")
        logger.info(f"æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’")
        if overall_success:
            logger.info("ğŸ‰ æ‰€æœ‰å…³é”®ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
        else:
            logger.info("âš ï¸  éƒ¨åˆ†ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œä½†éå…³é”®ä»»åŠ¡ä¸å½±å“æ•´ä½“æµç¨‹")
        logger.info("=" * 60)
    def update_sales_statistics(self, days_back=30, result_type="1", date_unit="4", data_type="4", sids=None):
        """
        æ›´æ–°é”€é‡ç»Ÿè®¡æ•°æ®
        Args:
            days_back: è·å–æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®ï¼ˆé»˜è®¤30å¤©ï¼‰
            result_type: æ±‡æ€»ç±»å‹ 1é”€é‡ 2è®¢å•é‡ 3é”€å”®é¢
            date_unit: ç»Ÿè®¡æ—¶é—´æŒ‡æ ‡ 1å¹´ 2æœˆ 3å‘¨ 4æ—¥
            data_type: ç»Ÿè®¡æ•°æ®ç»´åº¦ 1ASIN 2çˆ¶ä½“ 3MSKU 4SKU 5SPU 6åº—é“º
            sids: åº—é“ºIDåˆ—è¡¨ï¼Œå¤šä¸ªä½¿ç”¨è‹±æ–‡é€—å·åˆ†éš”
        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆç¡®ä¿ä¸è¶…è¿‡90å¤©é™åˆ¶ï¼‰
            days_back = min(days_back, 90)  # APIé™åˆ¶æœ€å¤§90å¤©
            end_date = datetime.now().date()
            start_date = end_date - timedelta(days=days_back)
            start_date_str = start_date.strftime("%Y-%m-%d")
            end_date_str = end_date.strftime("%Y-%m-%d")
            print(f"å¼€å§‹æ›´æ–°é”€é‡ç»Ÿè®¡æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date_str} åˆ° {end_date_str}")
            print(f"ç»Ÿè®¡å‚æ•° - æ±‡æ€»ç±»å‹: {result_type}, æ—¶é—´å•ä½: {date_unit}, æ•°æ®ç»´åº¦: {data_type}")
            if sids:
                print(f"æŒ‡å®šåº—é“ºID: {sids}")
            # è°ƒç”¨é”€é‡æ•°æ®è·å–æ–¹æ³•
            success = self.api_client.get_sales_by_date_range(
                db_config=self.db_config,
                start_date=start_date_str,
                end_date=end_date_str,
                result_type=result_type,
                date_unit=date_unit,
                data_type=data_type,
                sids=sids
            )
            if success:
                print("é”€é‡ç»Ÿè®¡æ•°æ®æ›´æ–°æˆåŠŸ")
                # è®°å½•æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._log_sales_update_summary(start_date_str, end_date_str)
            else:
                print("é”€é‡ç»Ÿè®¡æ•°æ®æ›´æ–°å¤±è´¥")
            return success
        except Exception as e:
            print(f"æ›´æ–°é”€é‡ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False
    def _log_sales_update_summary(self, start_date, end_date):
        """
        è®°å½•é”€é‡æ•°æ®æ›´æ–°æ‘˜è¦ä¿¡æ¯ï¼ˆé€‚é…sales_codeï¼‰
        """
        try:
            if not self.data_operator or not self.data_operator.conn:
                self.connect_database()
            # æŸ¥è¯¢æœ¬æ¬¡æ›´æ–°çš„æ•°æ®ç»Ÿè®¡
            summary_sql = """
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT sales_code) as sales_code,
                SUM(volume_total) as total_volume,
                AVG(volume_total) as avg_volume,
                MIN(create_time) as earliest_record,
                MAX(create_time) as latest_record
            FROM sales_info 
            WHERE create_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
            AND create_time <= NOW()
            """
            self.data_operator.cursor.execute(summary_sql)
            result = self.data_operator.cursor.fetchone()
            if result and result[0] > 0:
                print(f"é”€é‡æ›´æ–°æ‘˜è¦ - æ—¶é—´æ®µ: {start_date} è‡³ {end_date}")
                print(f"  æ–°å¢è®°å½•æ•°: {result[0]} æ¡")
                print(f"  å”¯ä¸€SKUæ•°é‡: {result[1]} ä¸ª")
                print(f"  æ€»é”€é‡/é”€å”®é¢: {result[2]:.2f}")
                print(f"  å¹³å‡é”€é‡/é”€å”®é¢: {result[3]:.2f}")
                print(f"  æœ€æ—©è®°å½•æ—¶é—´: {result[4]}")
                print(f"  æœ€æ™šè®°å½•æ—¶é—´: {result[5]}")
            else:
                print("æœªæ‰¾åˆ°æœ¬æ¬¡æ›´æ–°çš„é”€é‡è®°å½•")
        except Exception as e:
            print(f"ç”Ÿæˆé”€é‡æ›´æ–°æ‘˜è¦å¤±è´¥: {e}")
    def cleanup_old_sales_data(self, days_to_keep=90):
        """
        æ¸…ç†æ—§çš„é”€é‡æ•°æ®
        """
        try:
            if not self.data_operator or not self.data_operator.conn:
                self.connect_database()
            # å…ˆç»Ÿè®¡è¦åˆ é™¤çš„æ•°æ®é‡
            count_sql = "SELECT COUNT(*) FROM sales_info WHERE create_time < DATE_SUB(NOW(), INTERVAL %s DAY)"
            self.data_operator.cursor.execute(count_sql, (days_to_keep,))
            count_result = self.data_operator.cursor.fetchone()
            if count_result and count_result[0] > 0:
                print(f"å‡†å¤‡æ¸…ç† {count_result[0]} æ¡ {days_to_keep} å¤©å‰çš„é”€é‡æ•°æ®")
                # æ‰§è¡Œåˆ é™¤
                delete_sql = "DELETE FROM sales_info WHERE create_time < DATE_SUB(NOW(), INTERVAL %s DAY)"
                self.data_operator.cursor.execute(delete_sql, (days_to_keep,))
                deleted_rows = self.data_operator.cursor.rowcount
                self.data_operator.conn.commit()
                print(f"æˆåŠŸæ¸…ç† {deleted_rows} æ¡æ—§é”€é‡æ•°æ®")
                # ä¼˜åŒ–è¡¨ç©ºé—´
                optimize_sql = "OPTIMIZE TABLE sales_info"
                self.data_operator.cursor.execute(optimize_sql)
                print("è¡¨ç©ºé—´ä¼˜åŒ–å®Œæˆ")
                return True
            else:
                print(f"æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§é”€é‡æ•°æ®ï¼ˆä¿ç•™ {days_to_keep} å¤©ï¼‰")
                return True
        except Exception as e:
            print(f"æ¸…ç†æ—§é”€é‡æ•°æ®å¤±è´¥: {e}")
            if self.data_operator.conn:
                self.data_operator.conn.rollback()
            return False
    def rebuild_orders_merge_table(self):
        """
        é‡å»ºè®¢å•åˆå¹¶å®½è¡¨ orders_merge
        å°†è®¢å•ã€ç‰©æµã€å•†å“å’Œåº—é“ºä¿¡æ¯è”åˆä¸ºä¸€ä¸ªå®½è¡¨ä¾›å‰ç«¯å±•ç¤º
        """
        try:
            print("å¼€å§‹é‡å»ºè®¢å•åˆå¹¶å®½è¡¨...")
            if not self.data_operator or not self.data_operator.conn:
                self.connect_database()
            # é‡å»ºåˆå¹¶è¡¨çš„SQLè¯­å¥
            sql = """
            DROP TABLE IF EXISTS orders_merge;
            CREATE TABLE orders_merge AS
            SELECT 
                o.global_order_no,
                o.reference_no,
                o.store_id,
                o.order_from_name,
                o.delivery_type,
                o.split_type,
                o.order_status,
                -- å°†æ—¶é—´å­—æ®µæ”¹ä¸ºDATETIMEç±»å‹
                CASE 
                    WHEN o.global_purchase_time IS NOT NULL AND o.global_purchase_time != 0 
                    THEN FROM_UNIXTIME(o.global_purchase_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_purchase_time,
                
                CASE 
                    WHEN o.global_payment_time IS NOT NULL AND o.global_payment_time != 0 
                    THEN FROM_UNIXTIME(o.global_payment_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_payment_time,
                
                CASE 
                    WHEN o.global_review_time IS NOT NULL AND o.global_review_time != 0 
                    THEN FROM_UNIXTIME(o.global_review_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_review_time,
                
                CASE 
                    WHEN o.global_distribution_time IS NOT NULL AND o.global_distribution_time != 0 
                    THEN FROM_UNIXTIME(o.global_distribution_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_distribution_time,
                
                CASE 
                    WHEN o.global_print_time IS NOT NULL AND o.global_print_time != 0 
                    THEN FROM_UNIXTIME(o.global_print_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_print_time,
                
                CASE 
                    WHEN o.global_mark_time IS NOT NULL AND o.global_mark_time != 0 
                    THEN FROM_UNIXTIME(o.global_mark_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_mark_time,
                
                CASE 
                    WHEN o.global_delivery_time IS NOT NULL AND o.global_delivery_time != 0 
                    THEN FROM_UNIXTIME(o.global_delivery_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_delivery_time,
                
                o.amount_currency,
                
                CASE 
                    WHEN o.global_latest_ship_time IS NOT NULL AND o.global_latest_ship_time != 0 
                    THEN FROM_UNIXTIME(o.global_latest_ship_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_latest_ship_time,
                
                CASE 
                    WHEN o.global_cancel_time IS NOT NULL AND o.global_cancel_time != 0 
                    THEN FROM_UNIXTIME(o.global_cancel_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_cancel_time,
                
                CASE 
                    WHEN o.update_time IS NOT NULL AND o.update_time != 0 
                    THEN FROM_UNIXTIME(o.update_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS update_time,
                
                o.order_tag,
                o.pending_order_tag,
                o.exception_order_tag,
                o.wid,
                o.warehouse_name,
                o.original_global_order_no,
                o.supplier_id,
                o.is_delete,
                o.order_custom_fields,
                
                CASE 
                    WHEN o.global_create_time IS NOT NULL AND o.global_create_time != 0 
                    THEN FROM_UNIXTIME(o.global_create_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS global_create_time,
                
                l.logistics_type_id,
                l.logistics_type_name,
                l.logistics_provider_id,
                l.logistics_provider_name,
                l.actual_carrier,
                l.waybill_no,
                l.pre_weight,
                l.pre_fee_weight,
                l.pre_fee_weight_unit,
                l.pre_pkg_length,
                l.pre_pkg_height,
                l.pre_pkg_width,
                l.weight,
                l.pkg_fee_weight,
                l.pkg_fee_weight_unit,
                l.pkg_length,
                l.pkg_width,
                l.pkg_height,
                l.weight_unit,
                l.pkg_size_unit,
                l.cost_currency_code,
                
                CASE 
                    WHEN l.pre_cost_amount IS NOT NULL
                    THEN CAST(REPLACE(REPLACE(l.pre_cost_amount, '-ï¿¥', ''), 'ï¿¥', '') AS DECIMAL(10,2))
                    ELSE NULL 
                END AS pre_cost_amount,
                
                l.cost_amount,
                
                CASE 
                    WHEN l.logistics_time IS NOT NULL AND l.logistics_time != 0 
                    THEN FROM_UNIXTIME(l.logistics_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS logistics_time,
                
                l.tracking_no,
                l.mark_no,
                i.global_item_no,
                i.item_id,
                i.platform_order_no,
                i.order_item_no,
                i.item_from_name,
                i.msku,
                i.local_sku,
                i.product_no,
                i.local_product_name,
                i.is_bundled,
                i.title,
                i.variant_attr,
                i.unit_price_amount,
                i.item_price_amount,
                i.quantity,
                i.platform_status,
                i.item_type,
                i.stock_cost_amount,
                i.wms_outbound_cost_amount,
                i.stock_deduct_id,
                i.stock_deduct_name,
                i.cg_price_amount,
                i.shipping_amount,
                i.wms_shipping_price_amount,
                i.customer_shipping_amount,
                i.discount_amount,
                i.customer_tip_amount,
                i.tax_amount,
                i.sales_revenue_amount,
                i.transaction_fee_amount,
                i.other_amount,
                i.customized_url,
                i.platform_subsidy_amount,
                i.cod_amount,
                i.gift_wrap_amount,
                i.platform_tax_amount,
                i.points_granted_amount,
                i.other_fee,
                
                CASE 
                    WHEN i.delivery_time IS NOT NULL AND i.delivery_time != 0 
                    THEN FROM_UNIXTIME(i.delivery_time, '%Y-%m-%d %H:%i:%s')
                    ELSE NULL 
                END AS delivery_time,
                
                i.source_name,
                i.data_json,
                i.item_custom_fields,
                s.sid AS store_sid,
                s.store_name AS store_full_name,
                s.platform_code AS store_platform_code,
                s.platform_name AS store_platform_name,
                s.currency AS store_currency,
                s.is_sync AS store_is_sync,
                s.status AS store_status,
                s.country_code AS store_country_code
            FROM orders o
            LEFT JOIN logistics_info l ON o.global_order_no = l.global_order_no
            LEFT JOIN item_info i ON o.global_order_no = i.global_order_no
            LEFT JOIN store_info s ON o.store_id = s.store_id;
            ALTER TABLE orders_merge ADD PRIMARY KEY (global_item_no);
            -- æ·»åŠ ç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
            CREATE INDEX idx_orders_merge_global_item_no ON orders_merge(global_item_no);
            CREATE INDEX idx_orders_merge_global_order_no ON orders_merge(global_order_no);
            CREATE INDEX idx_orders_merge_store_id ON orders_merge(store_id);
            """
            # æ‰§è¡ŒSQLè¯­å¥
            statements = [stmt.strip() for stmt in sql.split(';') if stmt.strip()]
            for statement in statements:
                try:
                    self.data_operator.cursor.execute(statement)
                    print(f"æ‰§è¡ŒSQLæˆåŠŸ: {statement[:100]}...")
                except Exception as e:
                    print(f"æ‰§è¡ŒSQLå¤±è´¥: {e}")
                    print(f"å¤±è´¥è¯­å¥: {statement}")
                    # ç»§ç»­æ‰§è¡Œå…¶ä»–è¯­å¥ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
            self.data_operator.conn.commit()
            print("è®¢å•åˆå¹¶å®½è¡¨é‡å»ºæˆåŠŸ")
            return True
        except Exception as e:
            print(f"é‡å»ºè®¢å•åˆå¹¶å®½è¡¨å¤±è´¥: {e}")
            if self.data_operator.conn:
                self.data_operator.conn.rollback()
            return False

    def rebuild_sales_summary_daily(self):
        """
        é‡å»ºé”€é‡æ±‡æ€»è¡¨ sales_summary_daily - å…¼å®¹MySQL 5.7ç‰ˆæœ¬
        """
        try:
            print("å¼€å§‹é‡å»ºé”€é‡æ±‡æ€»è¡¨...")

            if not self.data_operator or not self.data_operator.conn:
                self.connect_database()

            # ç¡®ä¿ä½¿ç”¨å­—å…¸æ¸¸æ ‡
            if not hasattr(self.data_operator.cursor, 'description') or not self.data_operator.cursor.description:
                # é‡æ–°åˆ›å»ºå­—å…¸æ¸¸æ ‡
                self.data_operator.cursor.close()
                self.data_operator.cursor = self.data_operator.conn.cursor(pymysql.cursors.DictCursor)

            # åˆ†æ­¥éª¤æ‰§è¡Œçš„SQLè¯­å¥ - å…¼å®¹MySQL 5.7
            sql_steps = [
                # 1. åˆ›å»ºè¡¨
                """
                CREATE TABLE IF NOT EXISTS sales_summary_daily (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    sku VARCHAR(255) NOT NULL,
                    store_name VARCHAR(255),
                    platform_name VARCHAR(255),
                    recent_3d_sales DECIMAL(15,2) DEFAULT 0,
                    recent_7d_sales DECIMAL(15,2) DEFAULT 0,
                    recent_15d_sales DECIMAL(15,2) DEFAULT 0,
                    recent_30d_sales DECIMAL(15,2) DEFAULT 0,
                    total_sales DECIMAL(15,2) DEFAULT 0,
                    last_sale_date DATE,
                    summary_date DATE NOT NULL,
                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE KEY unique_sku_store_date (sku, store_name, summary_date)
                )
                """,

                # 2. æ¸…ç©ºè¡¨
                "TRUNCATE TABLE sales_summary_daily",

                # 3. åˆ›å»ºä¸´æ—¶è¡¨å­˜å‚¨è§£æåçš„æ•°æ®
                """
                CREATE TEMPORARY TABLE temp_sales_data (
                    sales_id INT,
                    sku VARCHAR(255),
                    store_name VARCHAR(255),
                    platform_name VARCHAR(255),
                    sale_date DATE,
                    daily_sales DECIMAL(15,2),
                    volume_total DECIMAL(15,2)
                )
                """
            ]

            # æ‰§è¡Œå‰ä¸‰ä¸ªæ­¥éª¤
            for i, statement in enumerate(sql_steps, 1):
                try:
                    self.data_operator.cursor.execute(statement)
                    print(f"âœ… æ­¥éª¤{i}æ‰§è¡ŒæˆåŠŸ")
                except Exception as e:
                    print(f"âŒ æ­¥éª¤{i}æ‰§è¡Œå¤±è´¥: {e}")
                    if i == 3:  # ä¸´æ—¶è¡¨åˆ›å»ºå¤±è´¥ï¼Œå¯èƒ½æ˜¯å·²å­˜åœ¨
                        try:
                            self.data_operator.cursor.execute("DROP TEMPORARY TABLE IF EXISTS temp_sales_data")
                            self.data_operator.cursor.execute(statement)
                            print(f"âœ… é‡æ–°åˆ›å»ºä¸´æ—¶è¡¨æˆåŠŸ")
                        except Exception as e2:
                            print(f"âŒ é‡æ–°åˆ›å»ºä¸´æ—¶è¡¨å¤±è´¥: {e2}")

            # 4. ä½¿ç”¨Pythonå¤„ç†JSONæ•°æ®ï¼ˆå…¼å®¹MySQL 5.7çš„æ–¹æ³•ï¼‰
            try:
                # è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„æ•°æ®
                select_sql = """
                SELECT 
                    sales_id,
                    sku,
                    store_name,
                    platform_name,
                    date_collect,
                    volume_total
                FROM sales_info 
                WHERE date_collect IS NOT NULL 
                AND date_collect != '{}'
                AND date_collect != ''
                """

                self.data_operator.cursor.execute(select_sql)
                rows = self.data_operator.cursor.fetchall()

                summary_date = datetime.now().date()
                processed_count = 0

                print(f"ğŸ” æ‰¾åˆ° {len(rows)} æ¡éœ€è¦å¤„ç†çš„é”€å”®è®°å½•")

                for row in rows:
                    try:
                        # å®‰å…¨åœ°è·å–å­—æ®µå€¼ - å¤„ç†å…ƒç»„å’Œå­—å…¸ä¸¤ç§æƒ…å†µ
                        if isinstance(row, dict):
                            sales_id = row.get('sales_id')
                            sku_data = row.get('sku')
                            store_data = row.get('store_name')
                            platform_data = row.get('platform_name')
                            date_collect_str = row.get('date_collect')
                            volume_total = row.get('volume_total')
                        else:
                            # å¦‚æœæ˜¯å…ƒç»„ï¼ŒæŒ‰é¡ºåºè·å–
                            sales_id = row[0] if len(row) > 0 else None
                            sku_data = row[1] if len(row) > 1 else None
                            store_data = row[2] if len(row) > 2 else None
                            platform_data = row[3] if len(row) > 3 else None
                            date_collect_str = row[4] if len(row) > 4 else None
                            volume_total = row[5] if len(row) > 5 else None

                        if not date_collect_str:
                            continue

                        # å¤„ç†JSONå­—ç¬¦ä¸²
                        try:
                            if isinstance(date_collect_str, str):
                                # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„JSON
                                date_collect_str = date_collect_str.replace("'", '"')
                                date_collect = json.loads(date_collect_str)
                            else:
                                date_collect = date_collect_str
                        except (json.JSONDecodeError, TypeError) as e:
                            print(f"âŒ JSONè§£æå¤±è´¥: {e}, æ•°æ®: {str(date_collect_str)[:100]}")
                            continue

                        if not isinstance(date_collect, dict):
                            continue

                        # å¤„ç†æ¯ä¸ªæ—¥æœŸ-é”€é‡å¯¹
                        for date_str, sales_str in date_collect.items():
                            try:
                                # è½¬æ¢æ—¥æœŸ
                                sale_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                                sales_value = float(sales_str) if sales_str else 0

                                # æå–SKUå’Œåº—é“ºä¿¡æ¯
                                sku = extract_from_json(sku_data, sales_id, 'SKU')
                                store_name = extract_store_name(extract_from_json(store_data, 'æœªçŸ¥åº—é“º', 'åº—é“º'))
                                platform_name = extract_from_json(platform_data, 'æœªçŸ¥å¹³å°', 'å¹³å°')

                                # æ’å…¥ä¸´æ—¶è¡¨
                                insert_temp_sql = """
                                INSERT INTO temp_sales_data 
                                (sales_id, sku, store_name, platform_name, sale_date, daily_sales, volume_total)
                                VALUES (%s, %s, %s, %s, %s, %s, %s)
                                """

                                self.data_operator.cursor.execute(insert_temp_sql, (
                                    sales_id, sku, store_name, platform_name,
                                    sale_date, sales_value, float(volume_total or 0)
                                ))
                                processed_count += 1

                            except Exception as e:
                                print(f"âŒ å¤„ç†æ—¥æœŸ {date_str} å¤±è´¥: {e}")
                                continue

                    except Exception as e:
                        # å®‰å…¨åœ°è·å–sales_idç”¨äºé”™è¯¯æŠ¥å‘Š
                        sales_id_val = 'æœªçŸ¥'
                        try:
                            if isinstance(row, dict):
                                sales_id_val = row.get('sales_id', 'æœªçŸ¥')
                            else:
                                sales_id_val = row[0] if len(row) > 0 else 'æœªçŸ¥'
                        except:
                            pass

                        print(f"âŒ å¤„ç†è®°å½• {sales_id_val} å¤±è´¥: {e}")
                        continue

                print(f"âœ… æˆåŠŸè§£æ {processed_count} æ¡æ—¥æœŸé”€å”®è®°å½•")

                # 5. ä»ä¸´æ—¶è¡¨æ±‡æ€»æ•°æ®å¹¶æ’å…¥åˆ°ç›®æ ‡è¡¨
                summary_sql = """
                INSERT INTO sales_summary_daily 
                (sku, store_name, platform_name, recent_3d_sales, recent_7d_sales, 
                 recent_15d_sales, recent_30d_sales, total_sales, last_sale_date, summary_date)
                SELECT 
                    sku,
                    store_name,
                    platform_name,
                    COALESCE(SUM(CASE WHEN sale_date >= CURDATE() - INTERVAL 3 DAY THEN daily_sales ELSE 0 END), 0) as recent_3d_sales,
                    COALESCE(SUM(CASE WHEN sale_date >= CURDATE() - INTERVAL 7 DAY THEN daily_sales ELSE 0 END), 0) as recent_7d_sales,
                    COALESCE(SUM(CASE WHEN sale_date >= CURDATE() - INTERVAL 15 DAY THEN daily_sales ELSE 0 END), 0) as recent_15d_sales,
                    COALESCE(SUM(CASE WHEN sale_date >= CURDATE() - INTERVAL 30 DAY THEN daily_sales ELSE 0 END), 0) as recent_30d_sales,
                    MAX(volume_total) as total_sales,
                    MAX(sale_date) as last_sale_date,
                    CURDATE() as summary_date
                FROM temp_sales_data
                WHERE sale_date >= CURDATE() - INTERVAL 30 DAY
                GROUP BY sku, store_name, platform_name
                HAVING recent_30d_sales > 0
                """

                self.data_operator.cursor.execute(summary_sql)
                inserted_rows = self.data_operator.cursor.rowcount
                print(f"âœ… æˆåŠŸæ±‡æ€»å¹¶æ’å…¥ {inserted_rows} æ¡è®°å½•åˆ°é”€é‡æ±‡æ€»è¡¨")

                # 6. æ¸…ç†ä¸´æ—¶è¡¨
                self.data_operator.cursor.execute("DROP TEMPORARY TABLE IF EXISTS temp_sales_data")

                self.data_operator.conn.commit()

            except Exception as e:
                print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
                import traceback
                print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                self.data_operator.conn.rollback()
                return False

            print("âœ… é”€é‡æ±‡æ€»è¡¨é‡å»ºæˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒâŒ é‡å»ºé”€é‡æ±‡æ€»è¡¨å¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            if self.data_operator.conn:
                self.data_operator.conn.rollback()
            return False




    def run_daily_update(self, days_to_check=1, enable_cleanup=False,
                         update_orders=True, update_inventory=True, update_warehouse=True,
                         update_store=True, update_sales=True, sales_days_back=7,
                         rebuild_merge_table=True, rebuild_sales_summary=True):
        """
        æ‰§è¡Œæ¯æ—¥æ›´æ–°ä»»åŠ¡ï¼ˆæ•´åˆé”€é‡æ•°æ®æ›´æ–°ï¼‰
        Args:
            days_to_check: æ£€æŸ¥æœ€è¿‘å¤šå°‘å¤©çš„è®¢å•
            enable_cleanup: æ˜¯å¦å¯ç”¨æ•°æ®æ¸…ç†
            update_orders: æ˜¯å¦æ›´æ–°è®¢å•æ•°æ®
            update_inventory: æ˜¯å¦æ›´æ–°åº“å­˜ä¿¡æ¯
            update_warehouse: æ˜¯å¦æ›´æ–°ä»“åº“ä¿¡æ¯
            update_store: æ˜¯å¦æ›´æ–°åº—é“ºä¿¡æ¯
            update_sales: æ˜¯å¦æ›´æ–°é”€é‡æ•°æ®
            sales_days_back: é”€é‡æ•°æ®å›æº¯å¤©æ•°
            rebuild_merge_table: æ˜¯å¦é‡å»ºè®¢å•åˆå¹¶å®½è¡¨
            rebuild_sales_summary: æ˜¯å¦é‡å»ºé”€é‡æ±‡æ€»è¡¨
        Returns:
            bool: ä»»åŠ¡æ‰§è¡Œæ˜¯å¦æˆåŠŸ
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥æ•°æ®æ›´æ–°ä»»åŠ¡")
        logger.info("=" * 60)
        # æ‰“å°å½“å‰é…ç½®
        logger.info("ğŸ“‹ ä»»åŠ¡é…ç½®:")
        logger.info(f"  æ›´æ–°è®¢å•æ•°æ®: {update_orders}")
        logger.info(f"  æ›´æ–°åº“å­˜ä¿¡æ¯: {update_inventory}")
        logger.info(f"  æ›´æ–°ä»“åº“ä¿¡æ¯: {update_warehouse}")
        logger.info(f"  æ›´æ–°åº—é“ºä¿¡æ¯: {update_store}")
        logger.info(f"  æ›´æ–°é”€é‡æ•°æ®: {update_sales}")
        logger.info(f"  é‡å»ºåˆå¹¶å®½è¡¨: {rebuild_merge_table}")
        logger.info(f"  é‡å»ºé”€é‡æ±‡æ€»: {rebuild_sales_summary}")
        logger.info(f"  æ•°æ®æ¸…ç†: {enable_cleanup}")
        start_time = time.time()
        overall_success = True
        task_results = {}
        try:
            # 1. è¿æ¥æ•°æ®åº“
            if not self.connect_database():
                return False
            # 2. è·å–å¹¶æ›´æ–°è®¢å•æ•°æ®ï¼ˆæ–°å¢å‚æ•°æ§åˆ¶ï¼‰
            if update_orders:
                logger.info("å¼€å§‹æ›´æ–°è®¢å•æ•°æ®...")
                order_success = self.fetch_updated_orders(days_to_check)
                task_results["è®¢å•æ•°æ®"] = order_success
                if not order_success:
                    logger.error("è®¢å•æ•°æ®æ›´æ–°å¤±è´¥")
                    overall_success = False
                else:
                    logger.info("âœ… è®¢å•æ•°æ®æ›´æ–°æˆåŠŸ")
            else:
                logger.info("è·³è¿‡è®¢å•æ•°æ®æ›´æ–°")
                task_results["è®¢å•æ•°æ®"] = "è·³è¿‡"
            # 3. æ›´æ–°ä»“åº“ä¿¡æ¯
            if update_warehouse:
                logger.info("å¼€å§‹æ›´æ–°ä»“åº“ä¿¡æ¯...")
                warehouse_success = self.update_warehouse_info()
                task_results["ä»“åº“ä¿¡æ¯"] = warehouse_success
                if not warehouse_success:
                    logger.warning("ä»“åº“ä¿¡æ¯æ›´æ–°å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡")
                    overall_success = overall_success and False  # éå…³é”®ä»»åŠ¡ï¼Œä¸å¼ºåˆ¶å¤±è´¥
                else:
                    logger.info("âœ… ä»“åº“ä¿¡æ¯æ›´æ–°æˆåŠŸ")
            else:
                logger.info("è·³è¿‡ä»“åº“ä¿¡æ¯æ›´æ–°")
                task_results["ä»“åº“ä¿¡æ¯"] = "è·³è¿‡"
            # 4. æ›´æ–°åº—é“ºä¿¡æ¯
            if update_store:
                logger.info("å¼€å§‹æ›´æ–°åº—é“ºä¿¡æ¯...")
                store_success = self.update_store_info()
                task_results["åº—é“ºä¿¡æ¯"] = store_success
                if not store_success:
                    logger.warning("åº—é“ºä¿¡æ¯æ›´æ–°å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡")
                    overall_success = overall_success and False
                else:
                    logger.info("âœ… åº—é“ºä¿¡æ¯æ›´æ–°æˆåŠŸ")
            else:
                logger.info("è·³è¿‡åº—é“ºä¿¡æ¯æ›´æ–°")
                task_results["åº—é“ºä¿¡æ¯"] = "è·³è¿‡"
            # 5. æ›´æ–°åº“å­˜ä¿¡æ¯ï¼ˆéœ€è¦å…ˆæœ‰ä»“åº“ä¿¡æ¯ï¼‰
            if update_inventory:
                logger.info("å¼€å§‹æ›´æ–°åº“å­˜ä¿¡æ¯...")
                inventory_success = self.update_inventory_info()
                task_results["åº“å­˜ä¿¡æ¯"] = inventory_success
                if not inventory_success:
                    logger.warning("åº“å­˜ä¿¡æ¯æ›´æ–°å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡")
                    overall_success = overall_success and False
                else:
                    logger.info("âœ… åº“å­˜ä¿¡æ¯æ›´æ–°æˆåŠŸ")
            else:
                logger.info("è·³è¿‡åº“å­˜ä¿¡æ¯æ›´æ–°")
                task_results["åº“å­˜ä¿¡æ¯"] = "è·³è¿‡"
            # 6. æ›´æ–°é”€é‡æ•°æ®
            if update_sales:
                logger.info("å¼€å§‹æ›´æ–°é”€é‡æ•°æ®...")
                sales_success = self._update_daily_sales(sales_days_back, enable_cleanup)
                task_results["é”€é‡æ•°æ®"] = sales_success
                if not sales_success:
                    logger.warning("é”€é‡æ•°æ®æ›´æ–°å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡")
                    overall_success = overall_success and False
                else:
                    logger.info("âœ… é”€é‡æ•°æ®æ›´æ–°æˆåŠŸ")
            else:
                logger.info("è·³è¿‡é”€é‡æ•°æ®æ›´æ–°")
                task_results["é”€é‡æ•°æ®"] = "è·³è¿‡"
            # 7. é‡å»ºè®¢å•åˆå¹¶å®½è¡¨ï¼ˆä¾›å‰ç«¯å±•ç¤ºï¼‰
            if rebuild_merge_table:
                logger.info("å¼€å§‹é‡å»ºè®¢å•åˆå¹¶å®½è¡¨...")
                merge_success = self.rebuild_orders_merge_table()
                task_results["è®¢å•åˆå¹¶å®½è¡¨"] = merge_success
                if not merge_success:
                    logger.warning("è®¢å•åˆå¹¶å®½è¡¨é‡å»ºå¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡")
                    overall_success = overall_success and False
                else:
                    logger.info("âœ… è®¢å•åˆå¹¶å®½è¡¨é‡å»ºæˆåŠŸ")
            else:
                logger.info("è·³è¿‡è®¢å•åˆå¹¶å®½è¡¨é‡å»º")
                task_results["è®¢å•åˆå¹¶å®½è¡¨"] = "è·³è¿‡"
            # 8. é‡å»ºé”€é‡æ±‡æ€»è¡¨ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
            if rebuild_sales_summary:
                logger.info("å¼€å§‹é‡å»ºé”€é‡æ±‡æ€»è¡¨...")
                sales_summary_success = self.rebuild_sales_summary_daily()
                task_results["é”€é‡æ±‡æ€»è¡¨"] = sales_summary_success
                if not sales_summary_success:
                    logger.warning("é”€é‡æ±‡æ€»è¡¨é‡å»ºå¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡")
                    overall_success = overall_success and False
                else:
                    logger.info("âœ… é”€é‡æ±‡æ€»è¡¨é‡å»ºæˆåŠŸ")
            else:
                logger.info("è·³è¿‡é”€é‡æ±‡æ€»è¡¨é‡å»º")
                task_results["é”€é‡æ±‡æ€»è¡¨"] = "è·³è¿‡"
            # 9. éªŒè¯æ•°æ®ä¸€è‡´æ€§ï¼ˆå¯é€‰ï¼‰
            logger.info("å¼€å§‹éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
            consistency_success = self.validate_order_status_consistency()
            task_results["æ•°æ®ä¸€è‡´æ€§"] = consistency_success
            if not consistency_success:
                logger.warning("æ•°æ®ä¸€è‡´æ€§éªŒè¯å¤±è´¥")
                overall_success = overall_success and False
            else:
                logger.info("âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
            # 10. æ¸…ç†æ—§æ•°æ®
            if enable_cleanup:
                logger.info("å¼€å§‹æ¸…ç†æ—§æ•°æ®...")
                cleanup_success = self.cleanup_old_data()
                task_results["æ•°æ®æ¸…ç†"] = cleanup_success
                if not cleanup_success:
                    logger.warning("æ•°æ®æ¸…ç†å¤±è´¥")
                    overall_success = overall_success and False
                else:
                    logger.info("âœ… æ•°æ®æ¸…ç†å®Œæˆ")
            else:
                logger.info("è·³è¿‡æ•°æ®æ¸…ç†")
                task_results["æ•°æ®æ¸…ç†"] = "è·³è¿‡"
            # 11. è®¡ç®—æ‰§è¡Œæ—¶é—´å¹¶ç”ŸæˆæŠ¥å‘Š
            execution_time = time.time() - start_time
            self._generate_update_report(task_results, execution_time, overall_success)
        except Exception as e:
            logger.error(f"æ¯æ—¥æ›´æ–°ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            overall_success = False
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        finally:
            # ç¡®ä¿æ•°æ®åº“è¿æ¥è¢«å…³é—­
            self.disconnect_database()
        return overall_success
def main():
    try:
        logger.info("=" * 60)
        logger.info("æ¯æ—¥æ•°æ®æ›´æ–°ç³»ç»Ÿå¯åŠ¨")
        logger.info("=" * 60)
        # åŠ è½½é…ç½®
        config = load_config_from_env()
        # åˆ›å»ºæ›´æ–°å™¨å®ä¾‹
        updater = DailyOrderUpdater(
            app_id=config['app_id'],
            app_secret=config['app_secret'],
            db_config=config['db_config']
        )
        # æ‰§è¡Œæ•´åˆåçš„æ¯æ—¥æ›´æ–°ä»»åŠ¡
        success = updater.run_daily_update(
            days_to_check=1,  # æ£€æŸ¥æœ€è¿‘1å¤©çš„è®¢å•
            enable_cleanup=False,  # æ˜¯å¦å¯ç”¨æ•°æ®æ¸…ç†
            update_orders=False,  # æ›´æ–°è®¢å•ä¿¡æ¯
            update_inventory=False,  # æ›´æ–°åº“å­˜ä¿¡æ¯
            update_warehouse=False,  # æ›´æ–°ä»“åº“ä¿¡æ¯
            update_store=False,  # æ›´æ–°åº—é“ºä¿¡æ¯
            update_sales=True,  # æ›´æ–°é”€é‡æ•°æ®
            sales_days_back=7,  # é”€é‡æ•°æ®å›æº¯7å¤©
            rebuild_merge_table=False,  # é‡å»ºè®¢å•åˆå¹¶å®½è¡¨
            rebuild_sales_summary=True  # æ–°å¢ï¼šé‡å»ºé”€é‡æ±‡æ€»è¡¨
        )
        if success:
            logger.info("âœ… æ¯æ—¥æ•°æ®æ›´æ–°ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
            sys.exit(0)
        else:
            logger.warning("âš ï¸ éƒ¨åˆ†æ•°æ®æ›´æ–°ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            sys.exit(1)
    except Exception as e:
        logger.error(f"ğŸ’¥ ç³»ç»Ÿæ‰§è¡Œå‡ºç°æœªé¢„æœŸé”™è¯¯: {e}")
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        sys.exit(1)
if __name__ == "__main__":
    main()
